# Consul Architecture
Consul是一个由多个不同组件组成的复杂系统，为了帮助使用者和开发者了解其工作原理和模型，本章介绍其系统架构。
> 本章包含了consul内部实现的技术细节。在操作和使用consul时，这些细节并不是必须的。这些技术细节是为了帮助那些希望了解内部原理的用户，通过这些细节，用户不必去了解consul源码

## 涉及到的术语
在讨论架构之前，我们提供一个本文会用到的术语表：

- Agent - Agent会在Consul cluster的每个成员上运行。agent有两种运行模式：client/server 模式，通过consul agent命令可以启动agent。由于所有的节点都必须运行agent, 相比称节点为client或server, 称其为agent要简单的多，当然还有其它原因。所有的agent都有DNS和http接口，并且负责健康检查和服务同步。
- Client - Client是转发所有RPC请求到server的agent。Client是相对无状态的，唯一的后台活动是加入LAN gossip pool。其只需要很小的系统资源开销和很少的网络带宽占用。
- Server - Server是负责较多任务的agent. 其是Raft quorum的成员，维护集群状态，响应RPC的请求，与其它数据中心的WAN gossip交互，转发请求至leader或远程的数据中心。
- Datacenter - 尽管数据中心的定义非常清晰，但还是需要考虑一些细节。在EC2的使用中，多个可用去是否可以认为是一个数据中心的构成部分？我们对数据中心的定义是这样的：网络环境隔离，低延迟，高带宽。这排除了通过公网连接的情况，因此我们把单区域的多个可用区认为是单数据中心的组成部分。 
- Consensus - 在我们的文档中，Consensus指在leader选举中，节点达成一致，以及事务的排序达成一致。consensus在事务上的一致性是指在有限状态机上的一致性，而我们对consensus的定义是指复制状态机的一致性。Consensus在[Wikipedia](https://en.wikipedia.org/wiki/Consensus_(computer_science)有更多的描述，consul相关的描述在[Consensus Protocol](03.2.md)。
- Gossip - Consul建立在[Serf](https://www.serf.io/)的基础上，Serf支持完整的gossip协议，gossip协议被用于多种用途。Serf提供成员关系管理，failure检测和事件广播。关于gossip在consul中的更多应用参考[gossip documentation](https://www.consul.io/docs/internals/gossip.html)([中文](03.3.md))。在这里只要知道gossip涉及随机的节点之间通过UDP通信就足够了。
- LAN Gossip - LAN gossip pool由分布在同一网络或数据中心的节点组成。
- WAN Gossip - WAN gossip pool仅包含server模式的agent。 这些server通过因特网或广域网连接，分布在不同的数据中心。
- RPC - 远程过程调用。这种请求/响应机制，允许客户端发出服务器请求。

## Consul整体架构
![](images/03.1.png)

以下将图中的架构拆开来介绍每一部分。可以看到如图是2个数据中心。Consul对多数据中心这种常见的使用场景提供完美的支持。

在每个数据中心，都运行着server和client。由于随着server的增多，server达成共识的速度会越来越慢，所以使用consul时建议3到5台server, 以兼顾性能与服务可用性。然而，clent的数量没有任何限制，可以轻易扩充到成千上万个。

一个数据中心的所有节点都使用gossip协议，这意味着数据中心所有的节点在处于一个gossip池中。这样做的目的：首先，client无需配置server的地址，服务发现是自动完成。其次，节点服务的健康检查不是在server上，而是分布式的。这种健康检测方式比心跳检测更具扩展性。最后，作为消息层通知事件使用，如leader选举等重要的事件。

每个数据中心的server都是一个Raft peer set的组成部分。它们一起工作并选出负有更多职责的leader成员，leader负责处理所有的查询请求和事务。由于connsensus协议，事务必须复制到其它的peer。由于这种机制，当非leader成员收到RPC请求，它需要将请求转发给集群leader。

server节点同时是WAN gossip池的一部分。WAN池和LAN池是不同，前者对互联网的高延迟进行了优化，它只包含consul server节点。WAN池的目的是让数据中心之间以低接触的方式发现彼此的服务。上线一个新的数据中心，并加入已有的WAN gossip池是很容易的。由于所有的server都运行在这个池中，因此可以进行跨数据中的服务请求。当server收到对其它数据中心的请求时，它将其转发到对应数据中心的任一server上，然后server将请求结果转发到本地的server leader。

这种架构，使得数据中心的之间的耦合非常低，更由于失败检测机制，连接缓存和多路复用，跨数据中心的请求相对快速且可靠。

一般情况，consul数据在不同的数据中心是不同步的。当请求访问远程数据中心的资源时，本地consul server转发RPC请求至远程数据中心的consul server并返回结果。如果远程数据中心不可用，则当前请求的资源不可用，但这不影响本地数据中的服务。有一些的特殊情况，可以同步有限的数据子集，如使用consul内建的[ACL复制](https://www.consul.io/docs/guides/acl.html#outages-and-acl-replication)功能，或使用[consul-replicate](https://github.com/hashicorp/consul-replicate)这类外部工具。

## Getting in depth

At this point we've covered the high level architecture of Consul, but there are many more details for each of the subsystems. The [consensus protocol](https://www.consul.io/docs/internals/consensus.html) is documented in detail as is the [gossip protocol](https://www.consul.io/docs/internals/gossip.html). The [documentation](https://www.consul.io/docs/internals/security.html) for the security model and protocols used are also available.

For other details, either consult the code, ask in IRC, or reach out to the mailing list.

## Link

[目录](../../README.md)

[3.2.Consensus协议](03.2.md)

