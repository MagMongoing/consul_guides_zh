# Consensus协议
Consul使用[consensus protocol](https://en.wikipedia.org/wiki/Consensus_(computer_science)来保证一致性（[CAP中定义的一致性](https://en.wikipedia.org/wiki/CAP_theorem))。使用的Consensus是基于["Raft: In search of an Understandable Consensus Algorithm"](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)。用图形化的形式解释Raft，参考[The Secret Lives of Data](http://thesecretlivesofdata.com/raft)。


## Raft协议概览
Raft是基于[Paxos](https://en.wikipedia.org/wiki/Paxos_%28computer_science%29) consensus算法。和Paxos相比，Raft是拥有转态更少，更简单且更容易懂的算法。

在讨论Raft之前，需要了解一些关键术语:
- Log - Raft系统工作的最主要部分是日志记录(log entry)。数据一致性的问题可以分解为复制日志的问题。日志是有序的日志序列。日志的一致可以认为是所有的成员对日志的记录和顺序达成一致。
- FSM - [有限状态机](https://en.wikipedia.org/wiki/Finite-state_machine)。FSM是有限状态的集合，这些状态之间可以切换。随着新日志的应用，FSM在不同的状态之间进行转换。应用相同的日志序列，必须有相同的结果，这意味着日志应用的行为必须是确定的。
- Peer set - peer set必须是所有参与日志复制的成员的集合，对consul来说，所有的server节点都是本地数据中心的peer set的成员。
- Quorum - 一个quorum是一个peer set中的大多数成员: 对于数字n，quorum要求大于等于(n/2)+1。如果peer set有5个成员，我们需要3个成员形成quorum。如果一个集群中，有quorum个成员由于种种原因不可用，则集群是不可用的，并且新的日志也无法提交。
- Commit Entry - 日志如果在quorum个成员上持久存储，就可以在leader上可以提交。一旦提交，日志就会被应用到各个节点。
- Leader - 在任何时候，peer set都会选择一个节点作为leader。Leader负责记录新的日志记录，负责复制给follower，并管理何时提交记录。

Raft是一种很复杂的协议，在这里不做过多的介绍(想要了解更多的细节，参考[这篇文章](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf))。在这里，我们只做更加抽象的描述来说明Raft的工作原理。

Raft节点总是处于3中状态之中: follower, candidate, or leader。所有的节点初始化启动为follower，这种转态，所有的节点都一可以接收leader的复制日志和参加投票。如果在一段时间内没有收到entries，节点开始自举为candidate状态。进入candidate状态后，节点需要其它节点的投票。如果candidate节点获得了多数投票，则它被推举为leader。leader需要接收新的日志并复制日志到其它的follower。In addition, if stale reads are not acceptable, all queries must also be performed on the leader.

集群选举leader后，便可以开始接收新的日志记录。Client可以向leader发起写新日志的请求(从Raft的角度来看，a log engtry是不透明的二进制块)。leader收到请求后，将log engtry持久化存储并尝试复制给集群的多数follower成员。一旦log entry被认为可以提交，那它就可以在有限转态机上应用。FSM是一种应用程序，Consul中，我们使用[MemDB](https://github.com/hashicorp/go-memdb)来维护集群的状态。consul通过写锁来等待log entry的提交和应用。当与查询一致模式结合使用时，实现了写语意后的读取(This achieves read after write semantics when used with the [consistent](https://www.consul.io/api/index.html#consistent) mode for queries.)。

很明显，日志的无限增长也是有问题的。为此，Raft提供了一种机制，通过该机制可以对当前的状态做快照，并压缩日志。由于FSM的抽象，恢复FSM的状态和使用旧日志重放至相同的状态是没有区别的。这就允许Raft在某个时间捕获到FSM的转态后，就可以删除达到这种状态需要重放的日志。这种机制无需用户介入，可以避免磁盘空间的无限占用，并减少日志重放的时间。使用MemDB的一个优势是: 允许consul在做快照的过程中，仍然接受新的请求，大大提高服务的可用性。

当大多数节点服务是可用的，则服务就可用。如果多数节点不可用，则集群无法处理新的log entries和维护成员关系。例: 有2个peers: A和B。要达到多数，意味着2个节点都必须同意提交entry log。

一个3节点Raft集群允许一个节点不可用，5个节点的集群允许2个节点不可用。建议每个数据中心配置3或5个consul server。这样在不牺牲性能的前提下，最大限度提高服务的可用性。文末的表格概括了可能的集群规模及相应的容错能力。

性能方面，Raft可以和Paxos相比较。假设集群leader稳定，提交一次日志需要往返半个集群。因此性能受限于磁盘I/O和网络延迟。Consul不是一个高吞吐的写系统，根据硬件和网络配置，其能够处理成百上千的事务。

## Raft in Consul
Consul只有server节点参与Raft，并且是peer set的成员。所有的client节点转发请求至server节点。这样设计的部分原因是，随着更多成员加入peer set，要形成大多数会需要更多的节点。这样带来性能问题，你可能需要等待数百台机器在entry的提交上达成共识，而不是只有少数几台。

当启动集群时，一个consul server以bootstrap的模式启动，这种模式允许在其自举为leader。一旦leader确定后，其它的server可以在保持安全和一致性的前提下，加入peer set。通常，首次添加几台服务器后，bootstrap模式可以进行禁用。更多详情参考[这里](https://www.consul.io/docs/guides/bootstrapping.html)。

所有的server节点作为peer set的一部分维持集群运行，因此，它们都知道当前的leader节点。当非leader节点收到RPC请求，其将请求转发至leader节点。如果RPC请求是查询类型，意味着是只读的，leader根据当前FSM的状态返回结果。如果RPC请求是事务类型，则意味着它需要修改FSM状态，leader产生新的new entry，并使用Raft来应用entry。一旦log entry提交并应用，则本次事务完成。

由于Raft复制的性质，其性能对网络延迟非常敏感。由于这个原因，每个数据中心都有独立的leader和peer set。数据也分布在各个数据中心，所以每个leader也仅仅是对本地的数据中心的数据负责。当收到远程数据中心的请求时，请求会被转发给响应的leader。这种设计，在不牺牲性能的前提下，允许低延迟的事务和提高了服务的可用性。

## 一致性模式
相比所有对复制日志的写是通过Raft进行，读取则更加灵活。为了满足开发者的需要，Consul支持的3中不同的读一致性模式。

- default - Raft使用leader租赁模式，提供一个时间窗口，这窗口内假设其leader角色是稳定。然而，如果leader与其它的peer隔离，在老的leader持有租约期间，其它成员可能会选举新的leader。这意味者会同时有2个leader。由于旧的leader无法提交新的日志，所以不存在脑裂的风险。但是，如果老的leader上有查询请求，则此时的数据是可能是历史数据。默认的一致性模式仅依赖leader租赁，可能会将历史数据返回给client。我们这样做的折衷是因为读取很快，通常是强一致的，只在极少的场景中会出现读取历史数据。由于分区，老的leader会很快下线，因此出现读取历史旧数据的时间窗口非常有限。
- consistent - 这种模式强一致模式。它要求leader必须和大多数节点进行确认它仍然是leader。这引入了对所有server节点额外一轮的轮询。这种折衷的结果是结果总是强一致的，但因为多一轮轮询，增加了一些延迟。
- stale - 这种模式允许任何server为读取服务，无论它是否是leader。这意味读取的数据可能是历史数据，但是即使是历史数据，也是落后leader 50毫秒以内的数据。这种折衷的结果是读取快速且可扩展。这种模式允许在没有leader的情况下提供读取，这意味着在集群不可用时，仍然能够回应读取请求。

关于这些模式的使用，参考[HTTP API](https://www.consul.io/api/index.html)

## Deployment Table
下表列出了多种集群规格的quorum数量和容错能力。推荐部署3或5个server。单节点的server是非常不建议的，因为在服务故障时会导致数据的丢失。

Servers | Quorum Size | Failure Tolerance
------- | ----------- | -----------------
 1      | 1           | 0
 2      | 2           | 0
 3      | 2           | 1
 4      | 3           | 1
 5      | 3           | 2
 6      | 4           | 2
 7      | 4           | 3

## Link

[目录](../../README.md)

[上一节 Architecture](03.1.md)

[下一节 Gossip协议](03.3.md)



